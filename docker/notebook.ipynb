{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f06e75-0ef2-414b-87e4-ced58f1f57cc",
   "metadata": {},
   "source": [
    "# Openshift with WatsonX and PosgreSQL for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16a92d0f-9a71-4bc0-b309-7c168d377585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install --upgrade pip\n",
    "!pip install \"langchain==0.0.345\" \n",
    "!pip install wget \n",
    "!pip install sentence-transformers \n",
    "!pip install \"chromadb==0.3.26\" \n",
    "!pip install \"ibm-watson-machine-learning>=1.0.333\" \n",
    "!pip install pydantic==1.10.11\n",
    "!pip install python-dotenv\n",
    "!pip install typing-inspect==0.8.0\n",
    "#!pip install typing_extensions==4.5.0\n",
    "!pip install psycopg2-binary\n",
    "!pip install pypdf\n",
    "!pip install pgvector\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b10e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_community\n",
    "#!pip install langchain ibm-watson-machine-learning --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d9bee9-b9b8-4eb6-8dab-b1ed2bae0bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, getpass\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9db150df-72b0-46f4-ba35-068318ee621a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "credentials = {\n",
    "    #\"url\":  \"https://eu-de.ml.cloud.ibm.com\",\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": os.getenv(\"API_KEY\", None)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655a2fa4-6c2d-4430-87b4-f0435ed1c409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    project_id = os.environ[\"PROJECT_ID\"]\n",
    "except KeyError:\n",
    "    project_id = input(\"Please enter your project_id (hit enter): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d91af51-d208-4206-9aa6-0f5c7cf35639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "filename = 'state_of_the_union.txt'\n",
    "url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n",
    "if not os.path.isfile(filename):\n",
    "    wget.download(url, out=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ad11cea-c99e-4706-9466-61a0df55b306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "loader = TextLoader(filename ,encoding='utf-8')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34b173ed-72e7-48bc-936d-8d388d806ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1183f5d9-cce1-4d22-808d-8f9a43b169ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: vectordb\n",
      "Database: vectordb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "# Get the values from the .env file\n",
    "user = os.getenv(\"user\")\n",
    "password = os.getenv(\"password\")\n",
    "database = os.getenv(\"database\")\n",
    "server = os.getenv(\"server\")\n",
    "print(\"User:\", user)\n",
    "print(\"Database:\", database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "982aa1e7-87e3-4aae-8a17-0e3c4bf98cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct the connection string\n",
    "CONNECTION_STRING = f\"postgresql+psycopg://{user}:{password}@{server}/{database}\"\n",
    "#CONNECTION_STRING = f\"postgresql://{user}:{password}@{server}/{database}\"\n",
    "\n",
    "# Print the connection string\n",
    "#print(CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a076393-cce7-4a6b-aadf-1763011bea24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql+psycopg://testuser:testpwd@localhost:5432/vectordb\n"
     ]
    }
   ],
   "source": [
    "user = \"testuser\"\n",
    "password =\"testpwd\"\n",
    "database = \"vectordb\"\n",
    "#server = \"af651cca01b154fe28a0df0167cad5a7-844854289.us-east-2.elb.amazonaws.com\"\n",
    "server=\"localhost\"\n",
    "# Construct the connection string\n",
    "CONNECTION_STRING = f\"postgresql+psycopg://{user}:{password}@{server}:5432/{database}\"\n",
    "# Print the connection string\n",
    "print(CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3b12014-22f6-49ee-b34d-bf74aeb8b313",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=server,\n",
    "    database=database,\n",
    "    user=user,\n",
    "    password=password\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT 1\")\n",
    "print(cur.fetchone())  # Should print (1,)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f06d4695-a9ee-4397-8177-aee0b6777667",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://testuser:testpwd@localhost/vectordb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "\n",
    "# Construct the connection string\n",
    "CONNECTION_STRING = f\"postgresql://{user}:{password}@{server}/{database}\"\n",
    "print(CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc1bc605-5e96-4333-999b-4395c3585362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a connection to the database\n",
    "conn = psycopg2.connect(CONNECTION_STRING)\n",
    "\n",
    "# Create a cursor object to execute queries\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute the SQL command\n",
    "cur.execute(\"\"\"\n",
    "    CREATE EXTENSION IF NOT EXISTS vector;\n",
    "    CREATE TABLE IF NOT EXISTS embeddings (\n",
    "      id SERIAL PRIMARY KEY,\n",
    "      embedding vector,\n",
    "      text text,\n",
    "      created_at timestamptz DEFAULT now()\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cbfe2e0-e245-498b-a5c7-f9d96759e8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'embeddings' exists!\n",
      "Schema of table 'embeddings':\n",
      "  id: integer\n",
      "  embedding: USER-DEFINED\n",
      "  created_at: timestamp with time zone\n",
      "  text: text\n"
     ]
    }
   ],
   "source": [
    "# Create a connection to the database\n",
    "conn = psycopg2.connect(CONNECTION_STRING)\n",
    "\n",
    "# Create a cursor object to execute queries\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Check if the table exists\n",
    "cur.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'embeddings')\")\n",
    "table_exists = cur.fetchone()[0]\n",
    "\n",
    "if table_exists:\n",
    "    print(\"Table 'embeddings' exists!\")\n",
    "else:\n",
    "    print(\"Table 'embeddings' does not exist.\")\n",
    "\n",
    "# Get the schema of the table\n",
    "cur.execute(\"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'embeddings'\")\n",
    "schema = cur.fetchall()\n",
    "\n",
    "print(\"Schema of table 'embeddings':\")\n",
    "for column in schema:\n",
    "    print(f\"  {column[0]}: {column[1]}\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c964eb5a-34a2-4cc4-b55a-f98591f78849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66eabdce-f5a0-4f7d-b9fc-fba8251f0e15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [............................................................................] 230328 / 230328"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "pdf_folder_path = './rhods-doc'\n",
    "filename = 'Vector_database.pdf'\n",
    "url = 'https://github.com/ruslanmv/WatsonX-with-Langchain-PostgreSQL-with-pgvector/raw/master/rhods-doc/Vector_database.pdf'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(pdf_folder_path):\n",
    "    os.makedirs(pdf_folder_path)\n",
    "\n",
    "full_path = os.path.join(pdf_folder_path, filename)\n",
    "\n",
    "if not os.path.isfile(full_path):\n",
    "    wget.download(url, out=full_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c47a71b2-3ad1-4a32-90e2-3ccc763ad899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "loader = PyPDFDirectoryLoader(pdf_folder_path)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd09ecda-d04b-4674-8014-cf4e94b8ba3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024,\n",
    "                                               chunk_overlap=40)\n",
    "all_splits_pdfs = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b438848a-d093-4244-8742-a707fd81d8bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Vector database\\nA vector database management system (VDBMS) or simply vector database or vector store is a\\ndatabase that can store vectors (fixed-length lists of numbers) along with other data items. Vector databases\\ntypically implement one or more Approximate Nearest Neighbor  (ANN) algorithms,[1][2] so that one can\\nsearch the database with a query vector to retrieve the closest matching da tabase records.\\nVectors are mathematical representations of data in a high-dimensional space. In this space, each dimension\\ncorresponds  to a feature of the data, and tens of thous ands of dimensions might be used to represent\\nsophisticated data. A vector's position in this space represents its characteristics. Words, phrases, or entire\\ndocuments, and images, audio, and ot her types of data can all be vectorized.[3]\\nThese feature vectors may be computed from the raw data using machine learning methods such as feature\\nextraction algorithms, word embeddings[4] or deep learning networks. The goal is that semantically similar\", metadata={'source': 'rhods-doc\\\\Vector_database.pdf', 'page': 0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits_pdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "326d0b9b-f8da-45da-939b-5b591653b891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for doc in all_splits_pdfs:\n",
    "    doc.page_content = doc.page_content.replace('\\x00', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5236f1f-91a8-426b-92ab-719d2a58f984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rusla\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "COLLECTION_NAME = \"documents_test\"\n",
    "\n",
    "db = PGVector.from_documents(\n",
    "    documents=all_splits_pdfs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "921eb1b1-7b34-45f8-b368-76cb99763e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "42b536a4-07ee-489f-aa3e-9e44b4a65b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = ModelTypes.GRANITE_13B_CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2b2980c2-c110-4cfb-8c1c-be4dafe6157e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id ='ibm/granite-13b-chat-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a6a2323-8d27-47d4-b4ad-921fea3f284f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2510a9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, getpass\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a61727d4-1426-4d18-9101-aace3a845563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "credentials = {\n",
    "    #\"url\":  \"https://eu-de.ml.cloud.ibm.com\",\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": os.getenv(\"API_KEY\", None)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baaa1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3c83869-0674-4571-915f-b1f2af5ded17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of WatsonxLLM\n",
    "# WatsonxLLM initialization\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 1000,\n",
    "    GenParams.MIN_NEW_TOKENS: 50,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "129de63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 200,\n",
    "    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "426548d5-e626-491f-b3b1-26fafca0376a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain.llms import WatsonxLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a5acdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ibm_watsonx_ai in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (0.0.5)\n",
      "Requirement already satisfied: ibm-watson-machine-learning in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm_watsonx_ai) (1.0.333)\n",
      "Requirement already satisfied: requests in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-watson-machine-learning->ibm_watsonx_ai) (2.32.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-watson-machine-learning->ibm_watsonx_ai) (1.26.18)\n",
      "Requirement already satisfied: pandas<1.6.0,>=0.24.2 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-watson-machine-learning->ibm_watsonx_ai) (1.5.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-watson-machine-learning->ibm_watsonx_ai) (2024.6.2)\n",
      "Requirement already satisfied: lomond in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-watson-machine-learning->ibm_watsonx_ai) (0.3.3)\n",
      "Requirement already satisfied: tabulate in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-watson-machine-learning->ibm_watsonx_ai) (0.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-watson-machine-learning->ibm_watsonx_ai) (23.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-watson-machine-learning->ibm_watsonx_ai) (7.0.1)\n",
      "Requirement already satisfied: ibm-cos-sdk==2.11.* in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-watson-machine-learning->ibm_watsonx_ai) (2.11.0)\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.11.0 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-cos-sdk==2.11.*->ibm-watson-machine-learning->ibm_watsonx_ai) (2.11.0)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.11.0 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-cos-sdk==2.11.*->ibm-watson-machine-learning->ibm_watsonx_ai) (2.11.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-cos-sdk==2.11.*->ibm-watson-machine-learning->ibm_watsonx_ai) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-cos-sdk-core==2.11.0->ibm-cos-sdk==2.11.*->ibm-watson-machine-learning->ibm_watsonx_ai) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from pandas<1.6.0,>=0.24.2->ibm-watson-machine-learning->ibm_watsonx_ai) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from pandas<1.6.0,>=0.24.2->ibm-watson-machine-learning->ibm_watsonx_ai) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from requests->ibm-watson-machine-learning->ibm_watsonx_ai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from requests->ibm-watson-machine-learning->ibm_watsonx_ai) (3.7)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from importlib-metadata->ibm-watson-machine-learning->ibm_watsonx_ai) (3.17.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from lomond->ibm-watson-machine-learning->ibm_watsonx_ai) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ibm_watsonx_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c96f0923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "731fad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = ModelTypes.GRANITE_13B_CHAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d139fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ibm/granite-13b-chat-v1'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68bafc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install https://www.piwheels.org/simple/ibm-watsonx-ai/ibm_watsonx_ai-0.1.1-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25f9b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install https://www.piwheels.org/simple/ibm-watsonx-ai/ibm_watsonx_ai-1.0.6-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcb35cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ibm_watsonx_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32e7e59a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import ibm_watsonx_ai python package. Please install it with `pip install ibm_watsonx_ai`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages\\langchain_community\\llms\\watsonxllm.py:169\u001b[0m, in \u001b[0;36mWatsonxLLM.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mibm_watsonx_ai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfoundation_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelInference\n\u001b[0;32m    171\u001b[0m     credentials \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget_secret_value() \u001b[38;5;28;01mif\u001b[39;00m values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapikey\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m         ),\n\u001b[0;32m    197\u001b[0m     }\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ModelInference' from 'ibm_watsonx_ai.foundation_models' (c:\\Users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages\\ibm_watsonx_ai\\foundation_models\\__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m watsonx_granite \u001b[38;5;241m=\u001b[39m \u001b[43mWatsonxLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapikey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapikey\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:183\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    182\u001b[0m     emit_warning()\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages\\pydantic\\v1\\main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[1;32mc:\\Users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages\\pydantic\\v1\\main.py:1102\u001b[0m, in \u001b[0;36mvalidate_model\u001b[1;34m(model, input_data, cls)\u001b[0m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1102\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1104\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY))\n",
      "File \u001b[1;32mc:\\Users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages\\langchain_community\\llms\\watsonxllm.py:214\u001b[0m, in \u001b[0;36mWatsonxLLM.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    211\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwatsonx_model\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m watsonx_model\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import ibm_watsonx_ai python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install ibm_watsonx_ai`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m     )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import ibm_watsonx_ai python package. Please install it with `pip install ibm_watsonx_ai`."
     ]
    }
   ],
   "source": [
    "watsonx_granite = WatsonxLLM(\n",
    "    model_id=model_id.value,\n",
    "    url=credentials.get(\"url\"),\n",
    "    apikey=credentials.get(\"apikey\"),\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f139beaf-5338-4427-b764-2c9824e730e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ibm-generative-ai in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (0.1.17)\n",
      "Requirement already satisfied: urllib3<2 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-generative-ai) (1.26.18)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-generative-ai) (2.32.2)\n",
      "Requirement already satisfied: pydantic>=1.10.9 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-generative-ai) (1.10.11)\n",
      "Requirement already satisfied: python-dotenv>=1.0.0 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-generative-ai) (1.0.1)\n",
      "Requirement already satisfied: aiohttp>=3.8.4 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-generative-ai) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=0.2.5 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-generative-ai) (6.0.1)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-generative-ai) (0.27.0)\n",
      "Requirement already satisfied: aiolimiter>=1.1.0 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-generative-ai) (1.1.0)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from ibm-generative-ai) (4.66.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from aiohttp>=3.8.4->ibm-generative-ai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from aiohttp>=3.8.4->ibm-generative-ai) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from aiohttp>=3.8.4->ibm-generative-ai) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from aiohttp>=3.8.4->ibm-generative-ai) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from aiohttp>=3.8.4->ibm-generative-ai) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from aiohttp>=3.8.4->ibm-generative-ai) (4.0.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from httpx>=0.24.1->ibm-generative-ai) (3.7.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from httpx>=0.24.1->ibm-generative-ai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from httpx>=0.24.1->ibm-generative-ai) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from httpx>=0.24.1->ibm-generative-ai) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from httpx>=0.24.1->ibm-generative-ai) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->ibm-generative-ai) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from pydantic>=1.10.9->ibm-generative-ai) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from requests>=2.31.0->ibm-generative-ai) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from tqdm>=4.65.0->ibm-generative-ai) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\rusla\\.conda\\envs\\openshift\\lib\\site-packages (from anyio->httpx>=0.24.1->ibm-generative-ai) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ibm-generative-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a081c068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openshift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
