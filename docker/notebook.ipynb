{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f06e75-0ef2-414b-87e4-ced58f1f57cc",
   "metadata": {},
   "source": [
    "# Openshift with WatsonX and PosgreSQL for RAG\n",
    "### Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a10452-b23c-44be-879f-e5b62ff5855d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install --upgrade pip\n",
    "!pip install \"langchain==0.0.345\" \n",
    "!pip install wget \n",
    "!pip install sentence-transformers \n",
    "!pip install \"chromadb==0.3.26\" \n",
    "!pip install ibm-watson-machine-learning==1.0.359\n",
    "!pip install pydantic==1.10.11\n",
    "!pip install python-dotenv\n",
    "!pip install typing-inspect==0.8.0\n",
    "!pip install typing_extensions==4.8.0\n",
    "!pip install psycopg2-binary\n",
    "!pip install pypdf\n",
    "!pip install pgvector\n",
    "!pip install sentence-transformers\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f7945b-9b15-43fe-a2c4-1b066522ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n",
    "import os \n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain.llms import WatsonxLLM\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "from typing import Any, List, Mapping, Optional, Union, Dict\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from pydantic import BaseModel, Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10e151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "credentials = {\n",
    "    #\"url\":  \"https://eu-de.ml.cloud.ibm.com\",\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": os.getenv(\"API_KEY\", None)\n",
    "}\n",
    "\n",
    "try:\n",
    "    project_id = os.environ[\"PROJECT_ID\"]\n",
    "except KeyError:\n",
    "    project_id = input(\"Please enter your project_id (hit enter): \")\n",
    "    \n",
    "    \n",
    "import wget\n",
    "filename = 'state_of_the_union.txt'\n",
    "url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n",
    "if not os.path.isfile(filename):\n",
    "    wget.download(url, out=filename)    \n",
    "    \n",
    "    \n",
    "    \n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "loader = TextLoader(filename ,encoding='utf-8')\n",
    "documents = loader.load() \n",
    "\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "# Get the values from the .env file\n",
    "user = os.getenv(\"user\")\n",
    "password = os.getenv(\"password\")\n",
    "database = os.getenv(\"database\")\n",
    "server = os.getenv(\"server\")\n",
    "print(\"User:\", user)\n",
    "print(\"Database:\", database)\n",
    "\n",
    "# Construct the connection string\n",
    "CONNECTION_STRING = f\"postgresql+psycopg://{user}:{password}@{server}/{database}\"\n",
    "#CONNECTION_STRING = f\"postgresql://{user}:{password}@{server}/{database}\"\n",
    "# Print the connection string\n",
    "print(CONNECTION_STRING)\n",
    "user = \"testuser\"\n",
    "password =\"testpwd\"\n",
    "database = \"vectordb\"\n",
    "#server = \"af651cca01b154fe28a0df0167cad5a7-844854289.us-east-2.elb.amazonaws.com\"\n",
    "server=\"localhost\"\n",
    "# Construct the connection string\n",
    "CONNECTION_STRING = f\"postgresql+psycopg://{user}:{password}@{server}:5432/{database}\"\n",
    "# Print the connection string\n",
    "print(CONNECTION_STRING)\n",
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=server,\n",
    "    database=database,\n",
    "    user=user,\n",
    "    password=password\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT 1\")\n",
    "print(cur.fetchone())  # Should print (1,)\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "\n",
    "# Construct the connection string\n",
    "CONNECTION_STRING = f\"postgresql://{user}:{password}@{server}/{database}\"\n",
    "print(CONNECTION_STRING)\n",
    "\n",
    "\n",
    "# Create a connection to the database\n",
    "conn = psycopg2.connect(CONNECTION_STRING)\n",
    "# Create a cursor object to execute queries\n",
    "cur = conn.cursor()\n",
    "# Execute the SQL command\n",
    "cur.execute(\"\"\"\n",
    "    CREATE EXTENSION IF NOT EXISTS vector;\n",
    "    CREATE TABLE IF NOT EXISTS embeddings (\n",
    "      id SERIAL PRIMARY KEY,\n",
    "      embedding vector,\n",
    "      text text,\n",
    "      created_at timestamptz DEFAULT now()\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n",
    "# Create a connection to the database\n",
    "conn = psycopg2.connect(CONNECTION_STRING)\n",
    "\n",
    "# Create a cursor object to execute queries\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Check if the table exists\n",
    "cur.execute(\"SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'embeddings')\")\n",
    "table_exists = cur.fetchone()[0]\n",
    "\n",
    "if table_exists:\n",
    "    print(\"Table 'embeddings' exists!\")\n",
    "else:\n",
    "    print(\"Table 'embeddings' does not exist.\")\n",
    "\n",
    "# Get the schema of the table\n",
    "cur.execute(\"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'embeddings'\")\n",
    "schema = cur.fetchall()\n",
    "\n",
    "print(\"Schema of table 'embeddings':\")\n",
    "for column in schema:\n",
    "    print(f\"  {column[0]}: {column[1]}\")\n",
    "\n",
    "# Close the cursor and connection\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "66eabdce-f5a0-4f7d-b9fc-fba8251f0e15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"Vector database\\nA vector database management system (VDBMS) or simply vector database or vector store is a\\ndatabase that can store vectors (fixed-length lists of numbers) along with other data items. Vector databases\\ntypically implement one or more Approximate Nearest Neighbor  (ANN) algorithms,[1][2] so that one can\\nsearch the database with a query vector to retrieve the closest matching da tabase records.\\nVectors are mathematical representations of data in a high-dimensional space. In this space, each dimension\\ncorresponds  to a feature of the data, and tens of thous ands of dimensions might be used to represent\\nsophisticated data. A vector's position in this space represents its characteristics. Words, phrases, or entire\\ndocuments, and images, audio, and ot her types of data can all be vectorized.[3]\\nThese feature vectors may be computed from the raw data using machine learning methods such as feature\\nextraction algorithms, word embeddings[4] or deep learning networks. The goal is that semantically similar\" metadata={'source': 'rhods-doc\\\\Vector_database.pdf', 'page': 0}\n",
      "--------------------------------------------------------------------------------\n",
      "Score:  0.46342575550079346\n",
      "data items receive feature vectors that are close to each other.\n",
      "Vector databases can be used for similarity search, multi-modal search, recommendations engines, large\n",
      "langua ges models (LLMs), etc.[5]\n",
      "Vector databases are also used to implement Retrieval-Augmented Generation (RAG), a method to improve\n",
      "domain-specific respons es of large language models. Text documents describing the domain of interest are\n",
      "collected and for each document a feature vector (know n as an \"embedding\") is computed, typically using a\n",
      "deep learning network, and stored in a vector database. Given a user prompt, the feature vector of the\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Retrieval-Augmented Generation (RAG) is a method to improve domain-specific responses of large language models. It works by collecting text documents describing the domain of interest and computing a feature vector, or embedding, for each document using a deep learning network. These embeddings are then stored in a vector database. Given a user prompt, the feature vector of the relevant document is retrieved from the vector database and used to generate a more accurate and relevant response.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "pdf_folder_path = './rhods-doc'\n",
    "filename = 'Vector_database.pdf'\n",
    "url = 'https://github.com/ruslanmv/WatsonX-with-Langchain-PostgreSQL-with-pgvector/raw/master/rhods-doc/Vector_database.pdf'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(pdf_folder_path):\n",
    "    os.makedirs(pdf_folder_path)\n",
    "\n",
    "full_path = os.path.join(pdf_folder_path, filename)\n",
    "\n",
    "if not os.path.isfile(full_path):\n",
    "    wget.download(url, out=full_path)\n",
    "    \n",
    "    \n",
    "\n",
    "loader = PyPDFDirectoryLoader(pdf_folder_path)\n",
    "docs = loader.load()    \n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024,\n",
    "                                               chunk_overlap=40)\n",
    "all_splits_pdfs = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "print(all_splits_pdfs[0])\n",
    "for doc in all_splits_pdfs:\n",
    "    doc.page_content = doc.page_content.replace('\\x00', '')\n",
    "    \n",
    "    \n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "COLLECTION_NAME = \"documents_test\"\n",
    "\n",
    "db = PGVector.from_documents(\n",
    "    documents=all_splits_pdfs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection_string=CONNECTION_STRING,)    \n",
    "\n",
    "\n",
    "model_id ='ibm/granite-13b-chat-v2'\n",
    "\n",
    "\n",
    "import os, getpass\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "credentials = {\n",
    "    #\"url\":  \"https://eu-de.ml.cloud.ibm.com\",\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": os.getenv(\"API_KEY\", None)\n",
    "}\n",
    "\n",
    "\n",
    "# Create an instance of WatsonxLLM\n",
    "# WatsonxLLM initialization\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 1000,\n",
    "    GenParams.MIN_NEW_TOKENS: 50,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 200,\n",
    "    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n",
    "}\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "creds = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": os.getenv(\"API_KEY\", None)\n",
    "}\n",
    "#!pip install ibm_watsonx_ai\n",
    "model_id ='ibm/granite-13b-chat-v2'\n",
    "\n",
    "\n",
    "watsonx_granite = WatsonxLLM(\n",
    "    model_id='ibm/granite-13b-chat-v2',\n",
    "    url=credentials.get(\"url\"),\n",
    "    apikey=credentials.get(\"apikey\"),\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=watsonx_granite, chain_type=\"stuff\", retriever=db.as_retriever())\n",
    "\n",
    "\n",
    "\n",
    "query = \"What is vector database?\"\n",
    "qa.run(query)\n",
    "\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024,\n",
    "                                               chunk_overlap=40)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "for doc in all_splits:\n",
    "    doc.page_content = doc.page_content.replace('\\x00', '')\n",
    "    \n",
    "    \n",
    "    \n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "store = PGVector(\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embeddings)    \n",
    "\n",
    "\n",
    "store.add_documents(all_splits_pdfs);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query = \"What is  Retrieval-Augmented Generation?\"\n",
    "docs_with_score = store.similarity_search_with_score(query)\n",
    "\n",
    "\n",
    "\n",
    "for doc, score in docs_with_score[:1]:\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Score: \", score)\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    \n",
    "    \n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=watsonx_granite, chain_type=\"stuff\", retriever=store.as_retriever())    \n",
    "\n",
    "\n",
    "query = \"What is Prompt?\"\n",
    "qa.run(query)\n",
    "\n",
    "\n",
    "query = \"What ist Retrieval-Augmented Generation?\"\n",
    "qa.run(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (OpenShift)",
   "language": "python",
   "name": "openshift"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
